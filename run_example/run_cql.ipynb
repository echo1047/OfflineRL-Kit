{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: May 20 2022 19:45:31\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from offlinerlkit.nets import MLP\n",
    "from offlinerlkit.modules import ActorProb, Critic, TanhDiagGaussian\n",
    "from offlinerlkit.utils.load_dataset import qlearning_dataset\n",
    "from offlinerlkit.buffer import ReplayBuffer\n",
    "from offlinerlkit.utils.logger import Logger, make_log_dirs\n",
    "from offlinerlkit.policy_trainer import MFPolicyTrainer\n",
    "from offlinerlkit.policy import CQLPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--algo-name\", type=str, default=\"cql\")\n",
    "    parser.add_argument(\"--task\", type=str, default=\"maze2d-umaze-v1\") # hopper-medium-v2\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\"--hidden-dims\", type=int, nargs='*', default=[256, 256, 256])\n",
    "    parser.add_argument(\"--actor-lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--critic-lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--tau\", type=float, default=0.005)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--target-entropy\", type=int, default=None)\n",
    "    parser.add_argument(\"--auto-alpha\", default=True)\n",
    "    parser.add_argument(\"--alpha-lr\", type=float, default=1e-4)\n",
    "\n",
    "    parser.add_argument(\"--cql-weight\", type=float, default=5.0)\n",
    "    parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--max-q-backup\", type=bool, default=False)\n",
    "    parser.add_argument(\"--deterministic-backup\", type=bool, default=True)\n",
    "    parser.add_argument(\"--with-lagrange\", type=bool, default=False)\n",
    "    parser.add_argument(\"--lagrange-threshold\", type=float, default=10.0)\n",
    "    parser.add_argument(\"--cql-alpha-lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--num-repeat-actions\", type=int, default=10)\n",
    "    \n",
    "    parser.add_argument(\"--epoch\", type=int, default=1000)\n",
    "    parser.add_argument(\"--step-per-epoch\", type=int, default=1000)\n",
    "    parser.add_argument(\"--eval_episodes\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=256)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args=get_args()):\n",
    "    # create env and dataset\n",
    "    env = gym.make(args.task)\n",
    "    dataset = qlearning_dataset(env)\n",
    "    # See https://github.com/aviralkumar2907/CQL/blob/master/d4rl/examples/cql_antmaze_new.py#L22\n",
    "    if 'antmaze' in args.task:\n",
    "        dataset.rewards = (dataset.rewards - 0.5) * 4.0\n",
    "    args.obs_shape = env.observation_space.shape\n",
    "    args.action_dim = np.prod(env.action_space.shape)\n",
    "    args.max_action = env.action_space.high[0]\n",
    "\n",
    "    # seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env.seed(args.seed)\n",
    "\n",
    "    # create policy model\n",
    "    actor_backbone = MLP(input_dim=np.prod(args.obs_shape), hidden_dims=args.hidden_dims)\n",
    "    critic1_backbone = MLP(input_dim=np.prod(args.obs_shape) + args.action_dim, hidden_dims=args.hidden_dims)\n",
    "    critic2_backbone = MLP(input_dim=np.prod(args.obs_shape) + args.action_dim, hidden_dims=args.hidden_dims)\n",
    "    dist = TanhDiagGaussian(\n",
    "        latent_dim=getattr(actor_backbone, \"output_dim\"),\n",
    "        output_dim=args.action_dim,\n",
    "        unbounded=True,\n",
    "        conditioned_sigma=True\n",
    "    )\n",
    "    actor = ActorProb(actor_backbone, dist, args.device)\n",
    "    critic1 = Critic(critic1_backbone, args.device)\n",
    "    critic2 = Critic(critic2_backbone, args.device)\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n",
    "    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n",
    "\n",
    "    if args.auto_alpha:\n",
    "        target_entropy = args.target_entropy if args.target_entropy \\\n",
    "            else -np.prod(env.action_space.shape)\n",
    "\n",
    "        args.target_entropy = target_entropy\n",
    "\n",
    "        log_alpha = torch.zeros(1, requires_grad=True, device=args.device)\n",
    "        alpha_optim = torch.optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "        alpha = (target_entropy, log_alpha, alpha_optim)\n",
    "    else:\n",
    "        alpha = args.alpha\n",
    "\n",
    "    # create policy\n",
    "    policy = CQLPolicy(\n",
    "        actor,\n",
    "        critic1,\n",
    "        critic2,\n",
    "        actor_optim,\n",
    "        critic1_optim,\n",
    "        critic2_optim,\n",
    "        action_space=env.action_space,\n",
    "        tau=args.tau,\n",
    "        gamma=args.gamma,\n",
    "        alpha=alpha,\n",
    "        cql_weight=args.cql_weight,\n",
    "        temperature=args.temperature,\n",
    "        max_q_backup=args.max_q_backup,\n",
    "        deterministic_backup=args.deterministic_backup,\n",
    "        with_lagrange=args.with_lagrange,\n",
    "        lagrange_threshold=args.lagrange_threshold,\n",
    "        cql_alpha_lr=args.cql_alpha_lr,\n",
    "        num_repeart_actions=args.num_repeat_actions\n",
    "    )\n",
    "\n",
    "    # create buffer\n",
    "    buffer = ReplayBuffer(\n",
    "        buffer_size=len(dataset[\"observations\"]),\n",
    "        obs_shape=args.obs_shape,\n",
    "        obs_dtype=np.float32,\n",
    "        action_dim=args.action_dim,\n",
    "        action_dtype=np.float32,\n",
    "        device=args.device\n",
    "    )\n",
    "    buffer.load_dataset(dataset)\n",
    "\n",
    "    # log\n",
    "    log_dirs = make_log_dirs(args.task, args.algo_name, args.seed, vars(args))\n",
    "    # key: output file name, value: output handler type\n",
    "    output_config = {\n",
    "        \"consoleout_backup\": \"stdout\",\n",
    "        \"policy_training_progress\": \"csv\",\n",
    "        \"tb\": \"tensorboard\"\n",
    "    }\n",
    "    logger = Logger(log_dirs, output_config)\n",
    "    logger.log_hyperparameters(vars(args))\n",
    "\n",
    "    # create policy trainer\n",
    "    policy_trainer = MFPolicyTrainer(\n",
    "        policy=policy,\n",
    "        eval_env=env,\n",
    "        buffer=buffer,\n",
    "        logger=logger,\n",
    "        epoch=args.epoch,\n",
    "        step_per_epoch=args.step_per_epoch,\n",
    "        batch_size=args.batch_size,\n",
    "        eval_episodes=args.eval_episodes\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    policy_trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
